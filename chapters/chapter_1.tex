\newpage
\begin{center}
  \textbf{\large 1. Аналитический обзор }
\end{center}
\refstepcounter{chapter}
\addcontentsline{toc}{chapter}{1. Аналитический обзор }

\section{Использование подходов ИИ в робототехнике}

    В последнее время, методы искусственного интеллекта показывают выдающиеся результаты в решении различных робототехнических задач, таких как: манипуляция \cite{related-manipulation-1, related-manipulation-2}, передвижение квадропедов \cite{related-locomotion-1, related-locomotion-2}, управление дронами \cite{related-drone-1, related-drone-2}, многоагентное планирование \cite{related-multi-agent-1, related-multi-agent-2} и многих других. Выделяют два основных подхода обучения моделей искусственного интеллекта в робототехнике: обучение с подкреплением (англ. Reinforcment Learning) и имитационное обучение (англ. Imitation Learning). 

    \subsection{Обучение с подкреплением}

        Обучение с подкреплением (Reinforcement Learning, RL) — это обширная область исследований, изучающая автономное принятие последовательных решений.

        В формальной постановке задачи обучения с подкреплением существует агент (англ. agent), который взаимодействует с окружающей средой (англ. environment), принимает действия (англ. actions). За каждое действие агент получает награду (англ. reward) от среды. Агент задаёт процедуру выбора действия по доступным наблюдением (англ. observations), эту процедуру далее будем называть стратегией или политикой (англ. policy). Целью агента является максимизация кумулятивной награды. 
        
        Среда обычно формулируется как марковский процесс принятия решений (МППР):

        \begin{itemize}
            \item пространства состояний $S$ 
            \item множество действий $A$
            \item $T: S \times A \to S$ -- функция переходов
            \item $R: S \times A \to \mathbb{R}$  -- функция вознаграждений
            \item $\gamma$ -- дисконтирующий множитель
        \end{itemize}

        Агент выполняет действие в среде, используя функцию стратегии \ref{eq:policy}: 

        \begin{equation}
            \pi: S \to A
            \label{eq:policy}
        \end{equation}

        В момент времени $t$ агент находится в состониии $s_t \in S$, выбирая действие $a_t = \pi(s_t), a_t \in A$, агент переходит в состояние $s_{t + 1}$ исходя из распределения $p(s_{t + 1} | s_t, a_t)$, задающимся функцией перехода $T$, и получает награду $r_t = R(s_t, a_t)$. Суммарное дисконтированное вознаграждение имеет вид \ref{eq:cum_reward}: 

        \begin{equation}
            R = \sum_{t = 0}^\infty  \gamma^t r_t = r(s_0, a_0) + \gamma r(s_1, a_1) + \gamma^2 r(s_2, a_2) \dots
            \label{eq:cum_reward}
        \end{equation}

        Тогда, формальная цель агента -- максимизировать ожидаемую отдачу по стратегии $\pi$ \ref{eq:agent_goal}:

        \begin{equation}
            \mathbb{E}_{\pi} \sum_{t = 0}^\infty \gamma^t r_t
            \label{eq:agent_goal}
        \end{equation}

        Функция полезности состояния -- это ожидаемое будущее вознаграждение, получаемое агентом при применении стратегии $\pi$ из состояния $s$ \ref{eq:V}:

        \begin{equation}
            V^{\pi} (s) = \mathbb{E}_{\pi} \left[ \sum_{i = 1}^{\infty} \gamma^i r(s_{t + i}, a_{t + i} | s_t = s) \right]
            \label{eq:V}
        \end{equation}

        Функция полезности действия -- это ожидаемое будущее вознаграждение получаемое агентом при использовании стратегии $\pi$ для будущих состояний в эпизоде \ref{eq:Q}:

        \begin{equation}
            Q^{\pi} (s, a) = \mathbb{E}_{\pi} \left[ \sum_{i = 1}^{\infty} \gamma^i r(s_{t + i}, a_{t + i} | s_t = s, a_t = a) \right]
            \label{eq:Q}
        \end{equation}

        
    \subsection{Имитационное обучение}

        Постановка задачи имитационного обучения схода с постановкой задачи обучения с подкреплением, основное отличие заключается в том, что вместо использования явной функции вознаграждения $r_t = R(s_t, a_t)$ предполагается наличие набора демонстраций, предоставленных экспертом.

        Формально, если, аналогично в рамках парадигмы МППР, есть среда с функцией переходов $T: S \times A \to S$ с состоянием $s \in S$ и действием $a \in A$, то задача имитационного обучения заключается в использовании набора демонстраций $\Xi = \{ \xi_1, \dots, \xi_D\}$ экспертной стратегии $\pi^*$, для нахождения стратегии $\hat{\pi}^*$, которая имитирует экспертную стратегию.

        Одним из наиболее популярных методов имитационного обучения является подход клонирования поведения (англ. Behavior Cloning). Данный метод использует набор экспертных демонстраций $\xi \in \Xi$, чтобы определить стратегию $\pi$, которая бы имитировала эксперта. Это задача может быть решена в рамках задачи обучения с учителем, где разница между выходом стратегии и экспертными демонстрациями минимизируются относительно некоторой метрики. Формально, цель решить оптимизационную задачу \ref{eq:bc}:

        \begin{equation}
            \hat{\pi}^* = argmin_{\pi} \sum_{\xi \in \Xi} \sum_{s \in \xi} L (\pi(s), \pi^*(s)),
            \label{eq:bc}
        \end{equation}

        где $L$ - функция потерь, $\pi^*(s)$ -- действие экспертной стратегии в состоянии $s$, $\pi^* (s)$ -- аппроксимированная стратегия. 

        Ограничения применения подхода клонирования поведения заключается в том, что обучения происходит исключительно за счет экспертных демонстраций, которые могут неравномерно покрывать всё пространство состояний. Поэтому, найденная стратегия может обладать малой эффективностью в состояниях, которых не было в экспертных демонстрациях.
        

    \subsection{Обучение на синтетических данных}

        В рассмотренных выше подходов машинного обучения, среда может быть реальной или симуляционной. Симуляционная среда, симулятор -- это программно-моделируемое виртуальное пространство, которое с заданной точностью воспроизводит физические законы, динамику объектов и сенсорные данные реального мира.
    
         Использованием симуляторов в контексте применения методов машинного обучения в робототехнике обусловлено ресурсоёмким и небезопасным процессом сбора данных или взаимодействием со средой в реальных условиях. Альтернативой такому подходу и выступает генерация данных в симуляторе, где данные дешевые, а взаимодействие агента со средой и сбор данных не угрожает реальному оборудованию и персоналу. Несмотря на преимущества, основной проблемой данного подхода является так называемая проблема sim-to-real gap \cite{he2023bridging}, связанная с тем, что симулятор представляет собой аппроксимацию реального мира, и его воспроизведение процессов отличается от того, как аналогичные процессы происходят в реальности. Это приводит к тому, что модель во время обучения на данных из симулятора, учится оперировать в мире с одной динамикой, а затем запускается в условиях другой.

    \subsection{Решение проблемы sim-to-real}

        Существует множество решений описанной проблемы такие как: идентификация системы, доменная рандомизация, доменная адаптация и другие. Концептуальные иллюстрации основных подходов представлены на рис.~\ref{sim-to-real-app}. Подход доменной рандомизации обладает большей популярностью ввиду его простоты. Суть подхода подразумевает создание разнообразных сценариев обучения, путём варьирования параметров симуляции, таких как: физические и визуальные характеристики объектов и окружающей среды, освещение, кинематические и динамические параметры робота и другие. В таком случае, предполагается, что среда реального мира -- это лишь одна из вариаций исходной среды симулятора. Если модель будет эффективной на распределении вариаций сцен, генерируемой доменной рандомизацией, то она будет эффективна и в реальных условиях. Таким образом доменная рандомизация решает проблему sim-to-real, делая модель более робастной по отношению к различным сценариям.  

\section{Современные методы оценки робастности моделей}

        \subsection{The COLOSSEUM}
    
        Одним ярким примером такого метода для оценки робастности моделей является работа The COLOSSEUM \cite{pumacay2024colosseum}, целью которого является определение падения эффективности моделей при изменении параметров среды. Метод предоставляет собой программный модуль на базе платформы RLBench \cite{james2020rlbench} позволяющий варьировать 14 параметров среды, среди которых цвета, текстуры объектов, параметры освещения и другие. Изменяя описанные параметры можно изменить распределение выходных данных: $p(x_{test}) \neq p(x_{train})$, но при этом условная вероятность распределения действий остается такой же: $p(y_{test} | x_{test}) = p(y_{train} | x_{train})$. Эффективность моделей оценивается долей успешных эпизодов, где успех эпизода определяется задачей. Алгоритм работы метода можно описать следующим образом:
    
        \begin{enumerate}
            \item Генерация обучающего набора данных на исходной, не рандомизированной среде
            \item Обучение модели на сгенерированных данных, робастность которой будет оценивается
            \item Тестирование обученной модели в среде, с последовательным варьированием каждого из 14 параметров 
            \item Для каждого параметра производится расчет падения доли успешных эпизодов относительно доли успешных эпизодов на исходной, не рандомизированной среде
            \item Определению ключевых для работы модели параметров, варьирование которых сильнее других влияет на эффективность модели
        \end{enumerate}
    
        По данному алгоритму авторы работы оценили робастность 5 передовых моделей для манипуляции: R3M \cite{nair2022r3m}, MVP \cite{Radosavovic2022}, PerAct \cite{shridhar2022peract}, RVT \cite{goyal2023rvt}, VoxPoser \cite{huang2023voxposer}. По результатам, доля успешных эпизодов падает на $30-50\%$ при варьировании параметров среды, и на более $75\%$ при применении всех вариаций. Оказалось, что рассматриваемые модели наиболее уязвимы по отношению к параметрам освещенности, цвету объекта манипулирования и количеству отвлекающих объектов. 
    
        \subsection{KitchenShift}
            Аналогичный метод предоставляют авторы работы KitchenShift \cite{xing2021kitchenshift}. Отличие от работы The COLOSSEUM заключается в количестве параметров доступных для вариации, в данной работе их 7, среди которых: 3D модели и текстуры объектов, положение камеры, освещение, начальное состояние объектов и робота. Алгоритм:
    
            \begin{enumerate}
                \item Модель обучается задаче $\mathcal{T}_{train}$ в домене $\mathcal{D}_{train}$ с доступом к ограниченному набору демонстраций;
                \item Производится тестирование модели на задаче $\mathcal{T}_{train}$ на множестве доменов $\{\mathcal{D}_{shifted}\}$, которые были получены варьированием параметров среды.
            \end{enumerate}
    
        Согласно этому алгоритму, были оценены методы на основе имитационного обучения, по результатам метрики эффективности моделей значительно снизились под воздействием новых, смещенных параметров среды.
    
        \subsection{SIMPLER}
    
            Рассмотренные ранее работы предполагают обучения моделей, за которым следует тестирование. Однако, иногда требуется оценить робастность уже обученных моделей.
            Авторы работы SIMPLER \cite{li24simpler} предлагают решение данной проблемы, в контексте моделей, обученных на реальных данных. В таком случае, не целесообразно проводить тестирование в реальных условиях, так как это небезопасно и ресурсоемко, поэтому необходимо проводить валидацию в симуляторе. Стоит отметить, что в работах The COLOSSEUM и KitchenShift отличие между доменами $\mathcal{D}_{train}$ и $\mathcal{D}_{shifted}$ заключается в изменении одного или нескольких параметров симулятора, поэтому на этапе тестирования изменение эффективности модели по большому счету обусловлено исключительно влиянием варьируемого параметра. В случае с моделью, обученной на реальных данных, нужно обоснование, почему изменение эффективности модели в симуляторе было вызвано изменяемым параметром. Для того чтобы уменьшить рассогласование между сценой симулятора и реальным миром, были применены инструменты real-to-sim для создания реалистичной среды с точки зрения визуальных и физических составляющих. Эмпирическим путем было установлено, что эффективность модели в построенной сцене коррелирует с эффективностью модели в реальном мире при одних и тех же изменениях в параметрах. Таким образом, данная работа показывает, что в симуляторе можно оценивать изменение эффективности при варьировании параметров, даже для моделей обученных на реальных данных. 
    
        Наиболее перспективной является работа The Colosseum, так как она обладает большой масштабируемостью, а также включает в себя большее количество возможных рандомизаций. Также этим методом были исследованы передовые модели, эффективность которых заметно снизилась в других условиях, что говорит об актуальности данного подхода.
        
    \section{Обзор симуляционных платформ для тестирования робастности моделей}
    
        В данном разделе будут рассмотрены перспективные симуляторы для построения метода оценки робастности.
    
        \subsection{IsaacSim}
        IsaacSim \cite{nvidia_isaac_sim} -- это высокопроизводительная платформа для симуляции и обучения роботов, разработанная NVIDIA. Она позволяет проводить параллельное обучение агентов в виртуальных средах с использованием графических процессоров (GPU). Основные особенности IsaacGym включают:
        \begin{itemize}
            \item Поддержка физически точных симуляций с использованием движка PhysX.
            \item Возможность одновременного обучения тысяч агентов в одной среде.
            \item Высокая фото-реалистичность.
        \end{itemize}
        
        Инструменты предоставляемые IsaacSim позволяют создавать реалистичные сцены, которые упрощают решение проблемы sim-to-real и позволяют проводить оценку моделей, обученных на реальных данных, по примеру работы SIMPLER.
        
        \subsection{RLBench}
        RLBench \cite{james2020rlbench} -- это платформа для обучения с подкреплением, ориентированная на задачи манипуляции роботами. Она предоставляет набор из более чем 100 задач, таких как открывание дверей, перемещение объектов и сборка конструкций. Особенности RLBench включают:
        \begin{itemize}
            \item Большой набор предопределенных задач, которые можно использовать для тестирования и обучения моделей.
            \item Возможность генерации данных для обучения и тестирования моделей.
        \end{itemize}
        RLBench позволяет оценивать робастность моделей в условиях, близких к реальным, благодаря разнообразию задач и сценариев.
        
        \subsection{MuJoCo}
        MuJoCo \cite{todorov2012mujoco} -- это физический движок, широко используемый для моделирования сложных динамических систем. Основные преимущества MuJoCo:
        \begin{itemize}
            \item Высокая точность моделирования физических взаимодействий, включая контакты, трение и деформации.
            \item Возможность гибкой настройки параметров среды.
        \end{itemize}
        MuJoCo является одной из наиболее популярных платформ благодаря своей гибкости и точности.
    
        Каждая из рассмотренных платформ имеет свои уникальные особенности, однако, для задачи оценивания робастности наиболее уместным является симулятор IsaacSim, ввиду его повышенной точности и качества графики. 
    